---
title: "Causal Reasoning of LLMs"
format: html 
editor: visual
lang: de
author: 
  - name: "Lucas Mandelik"
    affiliation: "HWI Hamburg"
    email: "lucas.mandelik@studium.uni-hamburg.de"
  - name: "Alexander Lorenz"
    affiliation: "HWI Hamburg"
    email: "alexander.lorenz@studium.uni-hamburg.de"
  - name: "Jannik Svenson"
    affiliation: "HWI Hamburg"
    email: "jannik.svenson@studium.uni-hamburg.de"
date: "30. Juni 2024"
---
<!-- format: pptx -->
AKTUALISIERUNGSDATUM <br>
26. Juni 2024

## Inhaltsverzeichnis
1. [Einleitung](#einleitung)
2. [Zusammenfassung des zugrundeliegenden Artikels](#zusammenfassung)
3. [Grundlagen](#grundlagen)
4. [Parrots Paradox](#parrotsparadox)
5. [Beitrag](#beitrag)
6. [Schlussbetrachtung](#schlussbetrachtung)
7. [Lizenz](#lizenz)

## Einleitung
In diesem Projekt wird sich mit den kausalen Inferenzfähigkeiten von Large Language Models (LLM) befasst. Grundlage hiefür bietet der Artikel "A Critical Review of Causal Reasoning Benchmarks for Large Language Models" von Yang et al. aus dem Dezember 2023. In diesem Artikel werden verschiedene Benchmarks für die Fähigkeit von LLMs geprüft, kausale Inferenzen durchzuführen. Hierfür werden Vorschläge von mehreren Artikeln geprüft. Im nachhinein haben wir einige der vorgeschlagenen Benchmarks an UHHGPT 3.5 Turbo und UHHGPT 4 omni, welche uns durch die Universität zur Verfügung gestellt wurden und auf OpenAI's ChatGPT basieren, getestet. 

## Zusammenfassung des zugrundeliegenden Artikels
In der Einleitung des Artikels werden zuerst kausale Hierarchien eingeführt, also verschiedene, auf einander aufbauende Ebenen kausaler Asusagen. Mit zunehmender Höhe auf der Hierachie nimmt auch die Komplexität der Aufgaben zu, die vom LLM ausgeführt werden können sollen. Es werden zwei dieser Hierachien vorgestellt, die erste von Zhang et al. (2023), welche aus drei Ebenen besteht:
1. [Typ 1](#typ1)
Kausale Aussagen aufgrund von Informationen über Situation und Umgebung treffen.
2. [Typ 2](#typ2)
Aufgrund von Daten neues Wissen entdecken.
3. [Typ 3](#typ3)
Quantitaive Auswirkungen von Aktionen vorhersagen.

Laut dem Artikel können die meisten aktuellen Modelle den ersten Typ problemlos ausführen, haben allerdings Probleme mit den Typen 2 und 3(, allerdings ist zweifelhaft, wie aktuell diese Aussage mittlerweile ist). Die zweite vorgestellte Hierarchie, beschrieben als "Leiter der Kausalität", besteht ebenfalls aus drei Stufen:
1. [Sehen](#sehen)
Grundlegende statistische Zusammenhänge erkennen und beschreiben.
2. [Handeln](#handeln)
Effekte einer bestimmten Aktion vorhersagen.
3. [Vorstellen](#vorstellen)
Aussagen über alternative oder faktenwidrige Resultate tätigen.

Hier wird erneut festgestellt, dass die meisten Modelle nur die erste Stufe der Leiter erreichen. 

Anschließend wurde eine Literatur-Recherche durchgeführt, und verschiedene Ansätze für das  Benchmarking von kausaler Aussagefähigkeit verglichen und bewertet. 
Hierfür wurden zuerst solche Tests untersucht, welche kausale Aussagefähigkeit aufgrund von kontextualen Fragen bewertet haben. Yang et al. kritisieren, dass in diesen Fällen die Möglichkeiten des LLm's eingeschränkt werden, indem es beispielsweise nur zwischen den Optionen "Ursache" und "Wirkung" wählen muss - in manchen Fällen mit noch mehr Optionen - und dem Modell so bereits Optionen vorgegeben werden, wodurch kausale Aussagefähigkeit unzureichend getestet wird. Außerdem wurde vorgebracht, dass in einigen Fällen, die Leistung des Modells auf die Wahl der Sprache zurückzuführen ist, da mit leicht abgeänderter oder verallgemeinerter Sprache schlechtere Ergebnisse erziehlt wurden.

Darauffolgend wurde darauf hingewiesen, dass die Fähigkeit intuitive Aussagen zu treffen, wie ein Mensch sie mit "gesundem Menschenverstand" tätigen würde, eine notwendige, aber keine hinreichende Bedingung für kausale Aussagefähigkeit ist. Meistens würden die Modelle vorhandes Wissen Abrufen, und keine neuen Aussagen treffen.

Danach wurde auf Tests eingegangen, die sich mit höheren Levels von kausaler Inferenz beschäftigen. Als Beispiele wurden Geschichten genannt, die fiktive Elemente enthalten, in deren Kontext das LLM kausale Aussagen tätigen mussten. Dies stelle einen besseren und robusteren Test für die Modelle dar, da das Modell nicht nur durch Abruf alter Informationen eine Aussage tätigen kann, den Kontext neu interpretieren muss, entscheiden muss, welche Informationen aus der Geschichte relevant sind und diese zu einer Aussage zusammentragen muss. Ein Element, das gerne für solche Geschichten genutzt wird, ist die Zeitreise.

Eine weitere Gruppe von Tests befasst sich mit DAGs. Besser gesagt mit der Fähigkeit von LLM's zur causal graph discovery. Hier werden dem Modell eine Reihe von Aussagen über bedingte Unabhängigkeit gegeben, aus welchen das Modell einen DAG erstellen soll. Yang et al. schreiben, dass durch diese Art des testens alle drei Stufen der "Leiter der Kausalität" befriedigt werden, da hier Interventionen und faktenwidrige Aussagen miteinbezogen werden. Allerdings wird angemerkt, dass es besser sein könnte, ganze Nomen als Knotenpunkte der DAGs zu verwenden, und nicht nur einfache Buchstaben, da dies selbst für Menschen nicht unbedingt möglich wäre. Ebenfalls wird angemerkt, dass bereits Informationen über die kausalen Ausrichtungen in den Trainingsdaten vorhanden sein könnten, und das Modell somit wieder auf bereits vorhandenes Wissen zurückgreift, also keine kausale Aussage trifft. Selbst wenn man in das Stichwort miteinbezieht, dass sich das Modell eine hypothetische Welt vorstellen soll, ist es zweifelhaft, ob dieser Befehl auch befolgt wird. 



## Grundlagen
Zuerst kann einmal festgehalten werden, dass wir uns im folgenden hauptsächlich mit dem nachfolgenden oft debattierten Paper: XXXXX beschäftigen.

## ParrotsParadox
Einige Nutzer von LLMs haben bemerkt, dass LLMs nicht zwingend immer Kausalität erkennen können und sie z.B. auf einfache kausale Fragen falsch antworten.
In unserem Repository haben wir einige Fälle gesammelt, die zweifelhafte kausale Fähigkeiten von Chat GPT aufzeigen. Außerdem haben wir auch aus der Litertur bekannte Fragen, bei denen LLMs schonmal gescheitert sind, überprüft. Generell kann ChatGPT 4.0 und auch bedingt Chat GPT 3.5 Turbo diese Fragen gut beantworten, was aber nicht an dem kausalen Verständnis liegen muss, da diese Cases auch anders beantwortet werden könnten. Beispielsweise ist es denkbar, dass OpenAI diese Problem erkannt hat und entschieden hat bekannte Fälle und Fragen, die Sie von Nutzern mitbekommen, direkt z.B. durch "hardcoden" entfernen könnten. Außerdem könnten solche Fälle auch in den Trainingsdaten insbesondere von neueren LLMs vorhanden sein.

## Beitrag MLS
hier evtl. Haupteil

### Subsection 01

Hallo

### Subsection 02

Tschüss

## Schlussbetrachtung
eventuell elegant abschließen?

## Referenzen
[Linying Yang, Vik Shirvaikar, Oscar Clivio & Fabian Falck](https://openreview.net/forum?id=mRwgczYZFJ).



## Anhang

### Spaß über LLMs vor GAI
Nachfolgende Späße sind nicht despektierlich gegenüber LLMs oder GAI gemeint, sondern sollen helfen diese voranzubringen.

[ChatGPT Failures](https://github.com/giuven95/chatgpt-failures.git)

# Testing Ground
In diesem Dokument wird ein LaTeX-Dokument in Quarto eingebunden.

```{verbatim}
\documentclass{article}
\usepackage{lipsum} % Beispieltextpaket

\title{Beispiel LaTeX Dokument}
\author{Autor Name}
\date{\today}

\begin{document}

\maketitle

\section{Einleitung}
\lipsum[1-2] % Beispieltext

\section{Hauptteil}
\lipsum[3-4] % Beispieltext

\end{document}
```

{include, file="latex_files/test.tex", raw=true}


\begin{tabular}{|l|l|}\hline
Age & Frequency \\ \hline
18--25  & 15 \\
26--35  & 33 \\
36--45  & 22 \\ \hline
\end{tabular}




# demo stuff


\vspace{30pt}
## Quarto

Quarto enables you to weave together content and executable code into a finished document. To learn more about Quarto see <https://quarto.org>.

## Running Code

When you click the **Render** button a document will be generated that includes both content and the output of embedded code. You can embed code like this:

```{r}
1 + 1
```


You can add options to executable code like this

```{r}
#| echo: false
2 * 2
```

The `echo: false` option disables the printing of code (only output is displayed).


## Include figures

You can also include some figures, for example 

![](figures/logo.png){width=50%}


## Load data 

You can load data using relative paths. Here we load data from the subdirectory `data`

```{r}
load("data/experiment_data_counterfactual.rda")
head(df)
```


## Load R code 


```{r}
# Load the R code in the file `R/my_function.R"
source("R/my_function.R")

set.seed(1234)
x <- draw(10)
print(x)
```
<!-- Text -->

